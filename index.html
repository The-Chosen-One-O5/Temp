<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice AI with DeepSeek R1 and AssemblyAI</title>
    <style>
        body {
            font-family: 'Poppins', sans-serif;
            background-color: #2a2a2a;
            color: #fff;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
        }
        button {
            padding: 12px 24px;
            font-size: 16px;
            background: #8e2de2;
            border: none;
            color: #fff;
            border-radius: 8px;
            cursor: pointer;
            margin: 10px;
        }
        button:hover {
            background: #7a25c9;
        }
        #status, #transcription, #response {
            margin: 10px;
            font-size: 16px;
        }
    </style>
</head>
<body>
    <button id="startBtn">Start Listening</button>
    <button id="stopBtn" disabled>Stop Listening</button>
    <div id="status">Status: Idle</div>
    <div id="transcription">Transcription: </div>
    <div id="response">Response: </div>

    <script>
        // Replace these with your API keys
        const ASSEMBLYAI_API_KEY = dd73c7d0f80e4013940e0a494ff3de43;
        const DEEPSEEK_API_KEY = sk-32a8b88f56f84ca8b547012c8455f183;

        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const status = document.getElementById('status');
        const transcriptionDiv = document.getElementById('transcription');
        const responseDiv = document.getElementById('response');

        let socket;
        let mediaRecorder;
        let audioStream;

        // Start recording and connect to AssemblyAI WebSocket
        async function startListening() {
            try {
                status.textContent = 'Status: Connecting...';
                startBtn.disabled = true;
                stopBtn.disabled = false;

                // Get microphone access
                audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(audioStream, { mimeType: 'audio/webm' });

                // Connect to AssemblyAI WebSocket
                socket = new WebSocket(`wss://api.assemblyai.com/v2/realtime/ws?sample_rate=16000`, ['Authorization', ASSEMBLYAI_API_KEY]);

                socket.onopen = () => {
                    status.textContent = 'Status: Listening...';
                    mediaRecorder.start(250); // Send audio chunks every 250ms
                };

                socket.onmessage = async (event) => {
                    const data = JSON.parse(event.data);
                    if (data.message_type === 'FinalTranscript') {
                        const transcript = data.text;
                        transcriptionDiv.textContent = `Transcription: ${transcript}`;
                        if (transcript) {
                            await processWithDeepSeek(transcript);
                        }
                    }
                };

                socket.onerror = (error) => {
                    console.error('WebSocket error:', error);
                    status.textContent = 'Status: Error occurred';
                };

                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0 && socket.readyState === WebSocket.OPEN) {
                        const reader = new FileReader();
                        reader.onload = () => {
                            const base64Audio = reader.result.split(',')[1];
                            socket.send(JSON.stringify({ audio_data: base64Audio }));
                        };
                        reader.readAsDataURL(event.data);
                    }
                };

            } catch (error) {
                console.error('Start error:', error);
                status.textContent = 'Status: Failed to start';
                resetUI();
            }
        }

        // Stop recording and clean up
        function stopListening() {
            status.textContent = 'Status: Stopping...';
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }
            if (audioStream) {
                audioStream.getTracks().forEach(track => track.stop());
            }
            if (socket && socket.readyState === WebSocket.OPEN) {
                socket.close();
            }
            resetUI();
        }

        // Process transcript with DeepSeek R1
        async function processWithDeepSeek(transcript) {
            try {
                status.textContent = 'Status: Processing with DeepSeek R1...';
                const response = await fetch('https://api.deepseek.com/v1/chat/completions', {
                    method: 'POST',
                    headers: {
                        'Authorization': `Bearer ${DEEPSEEK_API_KEY}`,
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        model: 'deepseek-reasoner', // DeepSeek R1
                        messages: [{ role: 'user', content: transcript }],
                        max_tokens: 150
                    })
                });

                const data = await response.json();
                const reply = data.choices[0].message.content;
                responseDiv.textContent = `Response: ${reply}`;
                speakResponse(reply);
            } catch (error) {
                console.error('DeepSeek error:', error);
                responseDiv.textContent = 'Response: Error processing with DeepSeek';
                status.textContent = 'Status: Idle';
            }
        }

        // Convert response to speech using Web Speech API
        function speakResponse(text) {
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.lang = 'en-US';
            utterance.onend = () => {
                status.textContent = 'Status: Idle';
            };
            window.speechSynthesis.speak(utterance);
            status.textContent = 'Status: Speaking...';
        }

        // Reset UI state
        function resetUI() {
            startBtn.disabled = false;
            stopBtn.disabled = true;
            status.textContent = 'Status: Idle';
        }

        // Event listeners
        startBtn.addEventListener('click', startListening);
        stopBtn.addEventListener('click', stopListening);
    </script>
</body>
</html>
